-------------------S-------------------
SLAM现在可做的有几点：
1. 用CNN产生深度图，并用SLAM产生的位姿变换矩阵将上一帧的深度图变换为该帧的深度图，将此种方法生成的深度图和CNN直接产生的该帧深度图做比较。
2. 解决单目SLAM在场景深度突变时尺度不准的问题。可根据视频流中历史帧的信息来判断。

------------------BY-------------------
1. CNN-SLAM 平面检测、SLAM和CNN融合
2. Unsupervised Learning of Depth and Ego-Motion from Video 修改一下框架和内容
3. 用Faster R-CNN 检测动态物体，并在静态场景重建时剔除。
4. 语义SLAM：先用语义分割检测到物体，再用SLAM进行三维数据标注。
5. 研三学长：稠密地图中加定位。
6. XF：用DL提取特征点（有不少paper），并进行帧间匹配。
7. XF：给特征点加上语义信息，提高匹配准确性和成功率。
8. XF：借RNN利用视频流的历史信息，RNN有两个变种。目前如何定义输入是个问题。
9. XF：有contribution的一个方向。

建议：
1. 《机器学习》主要是讲基于统计的机器学习，和DL关系不大。想入手DL可直接看几篇相关经典论文，以及网上对这几篇论文的各种解读即可。


PPT：
1. 光机电算部分加一个“光电图像处理”
2. ORB-SLAM加一个SVO的深度滤波器？




好，我可以帮你鉴别这个老板是不是你要跟随的人？

他有了 更好的平台会不会想起你？

1，喜欢分享。

小事上喜欢分享的人，遇到大事也不会含糊。

他不吝把自己懂的东西，在独处时慢慢讲给你听。

他没有大道理，但是他会把刚做完的事儿给你讲一讲，他为什么这么做。

当然，前提是他有空。

你也别在他忙的焦头烂额的时候跑去问他一些百度一下，你就知道的问题。

2，不懂不装。

尽管他是老板，他在自己不懂得地方他会问你。

他会从小事上发现你的优点，赞扬你。

他会为了一件你能做成的事，权利范围内给你特权。




    实习期能不能拿高工资呢？能，但是都是一些执行的工作，可以轻松的统计出工作量，例如客服，例如整理文件，例如收集数据等等，这些工作一般没有太多技术含量，而且非常容易出效果，例如收集数据，一天能整理多少个表格是看的到的，能帮助公司做多少事都是看的到的，所以公司当然愿意付你较高工资了，因为付你工资再高，还能高到哪里去呢，比找一个正式员工便宜多了。

      但是这些执行类的工作能提高你什么核心能力呢，很容易被这样的工作养着，养到一定年龄的时候，你就觉得这个年龄不应该拿这样的工资了，可是在重新去找工作，你会发现你没有核心能力，只能被绑架在这个平台了。

      我特别认同一句话，送给大家：你三年后成为什么样子取决于你现在做什么。

汇总好你的问题，等别人闲暇的时候，再去请教。你可以多问问题，但不能不分时机地问问题，因为其实大家都很忙，都有自己的事情要做完；提前汇总好你的问题，等别人闲的时候再去请教，既高效又把给别人带来的麻烦减少到最小。

28、随时给你的上司汇报你的工作进度。
已完成的工作，汇报结果；未完成工作，汇报进度及所需支持；大部分上司都喜欢你随时汇报进度，而不是仅仅只会埋头苦干。

29、汇报工作的时候先讲结论和重点。
汇报工作不是侃大山，你得先挑结论和重点来讲，如果领导对你讲的内容感兴趣，他是会主动来跟你侃大山的。


不管是实习生也好，还是在职人员也好，有的时候都会很迷茫，用人单位是否是可以继续工作的，是否有发展的。其实只要通过几个问题就能找到自己的答案了：1、薪酬待遇是否可以接受；2、人际关系是否和谐；3、企业文化是否认同；4、是否能在单位得到提升（能力或是岗位）；5、其他因素是否符合自身要求（离家近、休息日、福利待遇）等。以上问题，只要有两个以满足了你的自身要求，就是可以再继续的。当然，在有更好的选择的前提下人，你是可以再走一步的。


----------------------------experience----------------------------


-------------------------------问题-------------------------------
1. 如果采用CNN-SLAM的框架，在GPU上能否达到实时（最少15fps）？
目前几个开源的SLAM框架本身可达到实时（30fps），但CNN和SLAM的结合主要在于关键帧。CNN目前能做到5fps，优化后估计能达到8fps。经过调研，ORB-SLAM2的关键帧是6fps左右，因此理论上CNN-SLAM可以达到实时。

2. 使用纯SLAM的方案，类似于ARKit，是否可行？
要求：实时、效果（比如将一个虚拟手机放在平面上）、弱纹理的环境。

问题分析：ARKit并不只有单目SLAM，还用到了IMU。目前调研了两个开源框架：ORB-SLAM2和VINS。
a. 实时：两个框架的AR Demo均能达到实时（即使在手机上）。
b. 效果：环境纹理丰富的情况下，SLAM系统运行很稳定。
c. 弱纹理环境：ORB-SLAM无法工作。


-----------------------------工作任务-----------------------------
一、目标：在没有特征点的区域放置方块。

二、思路：用方框选择放置区域，如果方框内特征点足够多，就不触发CNN系统，直接用SLAM；如果方框内特征点稀疏，就用CNN预测出深度图，并从深度图中的方框区域中采样一些点，根据相机位姿，变换到世界坐标中去，用来拟合平面。

三、关于图片的发送有两种方案：
1. 方框中特征点不够时再发送当前图片给CNN，等待CNN预测出深度图，再执行上述操作。
   分析：代码简单，相对容易，而且相机的位姿和深度图可以一一对应，但是可能会有延迟。
2. SLAM每隔一定时间发送一张图片（比如：发送关键帧），CNN一直做处理，始终保持文件夹中有1张深度图（多余的删除），SLAM需要的时候就去取，然后再执行上述操作。
   分析：在一定程度上能解决延迟问题，需要多线程，代码相对复杂，而且当前相机位姿未必与深度图对应。

四、 需要解决的问题：
1. SLAM初始化时就需要根据CNN的预测值，把尺度确定了。
2. 一个plane对象，它保存着当初拟合自己所用的那些mappoints和Tcw，疑问：这些mappoints以及Tcw会随时间改变吗？（如果不改变，问题就简单了）
   问题来源：当地图发生改变的时候，会重新计算每个平面，也就是重新计算每个平面的Model矩阵（glTpw），如果上述mappoints和Tcw不会随时间改变，那么重计算的意义何在？

五、尺度问题工作日志
1. 我们要做什么？
我们要把CNN预测的深度值转换到相机坐标下。也就是说对于某个特征点，CNN预测的深度值d，要能直接当成相机坐标系下的z来使用。它应该与SLAM三角化得到的深度值是一样的。

问题分析：
1. 单目SLAM具有尺度不确定性，那么ORB_SLAM到底是怎么固定尺寸的呢？（初步找到一些答案）
   答：以参考帧（第一帧）为世界坐标，找到所有特征点的深度中值，之后位姿的平移部分和地图点的坐标都要除以这个中值。（只是初始化的时候这么做吗？还有意义是什么？）



--------------------------问题总结---------------------------
1. CNN预测的像素深度值与SLAM三角化的像素深度值，它们之间的尺度怎么融合？
答：在初始化的时候，以CNN的估计深度为基准。

2. 从没有纹理的区域采样的点，用于拟合平面，需要把它们添加到地图中吗？如果加进去，如何维护它？
答：很困难，没有给出建议。

3. ORB-SLAM2在特征点少的时候会LOST，这个问题如何改善？
答：特征点太少的时候，ORB-SLAM不靠谱，LOST现象很难改善。建议可以尝试一下VINS，它在弱纹理情况下比ORB好，因为它的匹配是基于patch的，而不是单个点。

4. 在做ORB-SLAM实验的时候发现，改变场景会引起一些特别的现象，这个问题怎么解释？
答：移动特征点少的部分，场景不会变，系统可能把这些点当做外点剔除了。而移动特征点多的部分，系统判断相机变化了。其实就是特征点多少的问题，少的部分不管怎样都会被当做外点剔除，多的部分就会被用来计算位姿。所以跟相机动不动关系不大。


总结：
不挑剔的环境下，ORB-SLAM可以满足需求。挑剔的环境下，可以考虑VINS。（半）稠密SLAM中，DSO比LSD更新。SVO速度快，代码不太好，不能直接用。

相机最好用全局快门，帧率高一些的。


-------------------------工作总结----------------------------
一、核心问题：LSD-SLAM的建图能做到多好（准确）？如何调整能改善其建图效果？

二、实验过程：
1. 相机标定：
    对于可以【自动变焦】的相机，标定出来的参数每次都不一样。而且使用标定出来的参数，效果并不是很好，还不如直接使用数据集提供的参数。

2. LSD单目建图：
    做了很多建图的实验，详情可以看/home/bst2017/视频/LSD单目建图实验 文件夹下的视频。
    摄像头不要离场景太近，一米左右的距离建图效果比较好，也不要拍摄太远的地方。所建的地图中会有很多噪声，我想是深度估计错误比较严重，这些点被赋予了错误的深度坐标。
    初始化时建图还不错，但随着相机的运动会有越来越多的噪声，甚至还会出现同一物体不一致的现象。我觉得很有可能是相机自动变焦导致的。还有，标定参数对结果影响也很大。


三、思路：
1. 使用正确的相机标定参数：
    实验发现，不同的相机标定参数对建图的效果影响非常大。
    问题：按照LSD-SLAM提供的标定样例，似乎只需要畸变参数，而不需要相机内参（fx,fy,cx,cy），这一点很奇怪。

2. 动态参数调整：
我只实验过修改第一个参数minUseGrad，即控制梯度阈值的参数。它的默认值是5，将它调小，会有更多的点被检测到，它越大，被检测到的点越少。实验发现，检测到的点太少的话，很容易跟踪失败。调整它，对于地图的稠密程度有影响，它的最佳值应该视场景而定。
调整其他的参数，对实验结果应该也会有影响，具体影响如何，还不得而知。

3. 更换摄像头：
目前使用的这个摄像头，具有自动变焦功能。这对于SLAM系统肯定会有影响，应该换一个定焦相机做实验。


---------------------------CNN-SLAM源码研究方向-----------------------------
1. CNN训练模型需要初始化，这个过程时间较长，但是只需要做一次，找到可以放CNN初始化代码的地方
思路：在System.cpp中，初始化词袋模型之前的位置，可以初始化CNN。

2. CNN初始化后会给出模型接口，用于计算关键帧对应的深度图，找出可以衔接CNN接口与关键帧的代码段

3. 找出将CNN估计出的深度图用于地图更新的代码段



--------------------------------ORB-SLAM-----------------------------------
备注：该文档只针对单目

一、初始化
1.1 单目初始化需要两帧，第一帧称为mInitialFrame，初始帧的特征点数量必须要大于100.
1.2 第二帧的特征点数量也要大于100，也就是说只有连续两帧的特征点数量都大于100，才能进行初始化过程，否则返回第一步，重新寻找mInitialFrame。
1.3 满足条件二，进入初始化过程。初始化过程的第一步是匹配前两帧，如果匹配点少于100，返回第一步。
1.4 通过H模型或F模型进行单目初始化，得到两帧间相对位姿、初始MapPoints（通过三角化得到这些点的深度值）。
1.5 将第一帧作为SLAM系统的世界坐标系。
1.6 尺度归一化：将初始化的所有地图点根据深度值排序，取中间值作为归一化基准，将所有地图点的坐标都除以这个基准，位姿的平移部分也要除。
1.7 至此，单目初始化过程结束。


二、跟踪
两个部分：帧间匹配得到相机初始位姿、LocalMap匹配进一步优化位姿
2.1 帧间匹配：
2.1.1 如果SLAM系统OK，那么可能有两种匹配方式。
第一种：
如果运动跟踪模型不为空，那么使用运动模型进行跟踪。
a.首先根据匀速运动模型，通过前一帧的位姿预测当前帧的位姿。
b.利用前一帧的关键点对应的地图点，通过投影到当前帧，缩小搜索范围，找到当前帧的特征点对应的地图点。
c.找到匹配点之后，通过优化重投影误差，初步优化位姿。
d.如果优化后剩余的内点大于10，则跟踪成功，否则采用第二种方法。

第二种：
如果恒速运动模型不可用，或者第一种方法失败，就采用跟踪最近关键帧的方法。
a.直接将前一帧的位姿作为当前帧的位姿初始值。
b.将当前帧的关键点与关键帧的特征点进行匹配（BOW加速），将匹配成功的点的mappoints直接赋给当前帧
c.得到匹配点之后，通过最小化重投影误差，初步优化位姿。
d.如果最终内点数量大于10，就跟踪成功。

2.1.2 如果SLAM系统已经LOST，那么就不能通过预测或前一帧得到当前帧的位姿了，必须要重定位，需要PNP求解位姿
a.从关键帧数据库中挑选与当前帧相似的帧（通过BOW）
b.对于每一个候选关键帧，找到与当前帧的匹配点，然后通过PNP求解位姿。
c.有了初始位姿，就可以进行BA了，最小化重投影误差。如果内点数量大于50，就可以break了，认为重定位成功。

2.2 与局部地图匹配
什么是局部地图呢？包括关键帧，参考关键帧，以及由这些关键帧观测到的mappoints
第一步：更新LocalMap（LocalMap包括LocalKeyFrame和LocalMapPoints）
a.遍历当前帧的所有mappoints，并得到所有能观测到当前mappoints的关键帧，共视程度最高的关键帧作为当前帧的参考关键帧。
b.构造LocalKeyFrame（有三个策略）
首先，将a中得到的关键帧全都加进去
然后，将a中那些关键帧共视程度很高的关键帧也加进去
共视程度很高的有3种：
第一种：最佳共视的10帧（从10个里面取一个就可以了）
第二种：所有子关键帧（也取一个就行了）
第三种：父关键帧
（注意：LocalKeyFrame最多不能超过80帧，而且要注意防止重复添加）

c.更新LocalMapPoints
这个比较简单，将上一步得到的LocalKeyFrame的所有MapPoints全都添加进去即可（注意要防止重复添加）

第二步：在Localmap中查找在当前帧视野范围内的点，将这些点和当前帧的特征点进行投影匹配。
经过匹配后就得到了新的匹配点对。

第三步：将上一步得到的新的匹配点进行最小化重投影误差优化位姿，并且统计当前帧的MapPoints被观测到的数量（内点）

第四步：决定是否跟踪成功。正常情况下，至少需要30个内点，如果在1秒之内发生过重定位，至少需要50个内点才算跟踪成功。
也就是说最后的trackLocalMap对是否跟踪成功具有决定权。

2.3 跟踪成功后，检查是否要插入关键帧。
a.定位模式下不用插入关键帧，因为也不建图
b.局部地图被闭环检测使用，也不插入新的关键帧
c.地图中关键帧太多，且距离上一次重定位时间少于1s，也不插入关键帧
d.得到参考关键帧跟踪到的地图点的数量，判断各种阈值，很麻烦。


-----------------------------Project----------------------------
一、如何跑工程
方法一：按照桌面的【SLAM命令】文件，依次打开四个终端，输入相应命令。
方法二：在主文件夹中有两个.sh文件，用终端依次打开1.sh和2.sh文件即可。（2.sh文件默认使用USB摄像头输入图片，如使用手机输入，自行注释掉2.sh文件中的相应代码即可）

二、工程框架
2.1 输入
输入有两种方式，都是通过ROS实现的。一种是USB摄像头输入，另一种是手机端输入。不同的方式要用不同的ROS节点，代码中有详细注释。
另外，在手机输入模式下，除了图片数据，还会有其他的数据通信，比如手机把屏幕点击的坐标发送到PC端，PC把模型视图矩阵发送到手机端。

2.2 SLAM系统
ROS得到图片后，会调用SLAM系统，SLAM系统会返回当前图片的位姿、特征点、地图点等信息。这些信息会被传给ViewAR。

2.3 ViewAR（工程核心代码）
这个线程最直接的任务，就是利用从SLAM得到的信息，渲染虚拟物体。（如果是手机端，还要利用手机屏幕坐标信息）
为了正确地渲染虚拟物体，实际上要完成很多工作，核心都集中在Run函数中，稍后会详细介绍。

2.4 输出
如果是采用USB摄像头，可以理解为没有输出，因为结果会直接在ViewAR线程中使用。
但是如果采用手机作为输入输出，那么ViewAR线程需要将结果返回给手机端。手机端完成渲染需要PC端提供两个矩阵，分别是视图矩阵和模型矩阵，它们都是4*4的矩阵。
视图矩阵是由当前图像的位姿矩阵构成的，也就是说它每一帧都不同，所以每一帧都要发送。
模型矩阵指的是虚拟物体本身在世界坐标系中的位置，一旦虚拟物体固定了，它就是不变的，因此只要在放置的时候发送一次就可以了。

三、工程核心
3.1 平面检测
3.1.1 如何定义一个平面？ 
一个平面由法向量和中心点组成。（都是在世界坐标系下）
法向量指的是从世界坐标系的原点，发出一条射线，垂直于目标平面的向量。也就是说相互平行的平面，法向量是一样的。它在代码中可以由（nx,ny,nz）三个数表示。
因为我们的平面并非无限大，所以需要指定平面的中心，才能固定一个平面的位置。它也是一个世界坐标（ox,oy,oz）

3.1.2 如何检测一个平面？
工程中，我们有两种方法得到平面的法向量：
第一种：需要若干点（世界坐标），用这些点构成一个矩阵，SVD分解这个矩阵，就可以得到平面的三个参数--->法向量。
SLAM用的是这种方法，CNN在检测竖直平面的时候用的也是这种方法（但是由于CNN提取的若干点一致性太差，导致计算出来的法向量不准确，也就是平面会倾斜）

第二种：为了解决上述问题，当CNN检测出目标平面可能是水平面的时候，我们直接用SLAM检测出来的水平面法向量代替，因为所有水平面的法向量都一样！（当然如果之前没有计算好的法向量，就自己计算）
这里有几个问题：首先，这种方法只适用于水平面，竖直平面有很多朝向，所以法向量是不固定的。
其次，判断一个平面是否是水平面，需要合适的阈值来判断，这就可能造成一定的误判。

3.1.3 如何计算一个平面的中心坐标？
方法一：SLAM最原始的方法是把同一平面上的所有点的坐标相加，求均值。所以这样拟合出来的平面位置是随机的，不能指哪放哪。
方法二：只考虑目标点一定范围内（矩形框内）的那些特征点对应的3D点，与方法一同理，只是范围更小了，更加准确了。
方法三：前两种方法都是针对于有特征点的情况的，没有深度图的话，理论上是不能指哪放哪的。但如果是CNN拟合平面，意味着我们有深度图，那么我们可以知道图像中任意一个点的3D坐标，所以我们可以指哪放哪。

3.2 虚拟物体掉落
想要实现物体掉落的效果，有几个工作是必须要做的：
a. 检测平面是否改变了
b. 删除原来的平面
c. 计算新平面的位置

附加信息：一个平面拥有多个属性，包括：是否是CNN的平面、平面中心的2D/3D坐标、平面中心像素的颜色、平面的法向量、平面改变的次数，等等。
这些属性都存储在相应的Vector中，具体参考代码，有详细注释。

3.2.1 如何检测平面是否变化了？
主要根据平面中心点RGB颜色变化，同时用到了HSL颜色空间的约束。
RGB和H的阈值可以在代码中看到是如何设置的，之所以用到亮度L约束，是因为摄像头的曝光很难控制，当摄像头曝光过多的时候，很容易出现误检，因此，我设定当亮度变化过高的时候，就不做检测。检测的频率是10fps，可以从界面控制。检测频率太高的话，帧间变化太小，容易检测不出来。

3.2.2 如何维护已检测的平面，以及如何删除变化的平面？
所有检测出来的平面以及平面的属性，都存放在相应的vector中。当我们检测到一个平面变化的时候，就将vector中的这个元素置为空，然后让它指向新创建的平面。

3.2.3 如何检测新平面的位置？
目前的方案，只能实现虚拟物体在相同法向量方向的掉落，也就是说掉落前后的平面必须平行。因此，我们只需要计算新平面的中心点就够了。
这里涉及到一些几何的推导。我画到纸上了。
