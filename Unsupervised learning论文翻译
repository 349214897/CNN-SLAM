Unsupervised learning of depth and ego-motion from video

[摘要]
    我们提出了一个无监督学习框架，它可从非结构化（即：未进行数据标注）的单目视频序列中估计出单目场景的深度和相机位姿。和最近[10,14,16]的工作相同，我们使用了一个端到端的学习方法，用视场合成(view synthesis)作为信号监督。和之前的工作相比，我们的方法是完全无监督的，只需要用单目视频序列训练即可。我们的方法运用了单视图深度(single-view depth)和多视图位姿网络(multi-view pose networks)，通过将当前帧图像结合预测的深度图以及帧间转移投影到临近帧上，计算像素误差作为训练的loss，对两个网络进行联合训练。两个网络在训练的时候通过loss耦合，但在测试时可以独立使用。在KITTI数据组以实验为依据的评估也证明了我们方法的有效性：1）单目深度估计效果很好，其效果与使用ground-truth位姿/深度的监督学习方法的效果差不多。2）该方法的位姿估计效果和基于SLAM的位姿估计效果差不多。


[1 引言]
    人类非常擅长判断自己的运动状态和周围三维场景结构，科学上的一种解释是我们在日常生活中经历了大量的视觉场景，并通过该过程在场景理解方面积累了丰富的经验。基于已有的经验，人们在新的未知场景下（甚至对单张单目图片）也能对场景有正确的结构化理解。而基于纯几何的SLAM等三维重建方法由于没有这种机器学习能力，因此在深度判断等场景理解任务上就必然会有无法避免的缺陷。
    如图1所示，我们模仿(mimic)上述过程，通过用图像序列训练网络，使其拥有估计相机运动和场景结构的能力。我们使用了一个端到端(end-to-end)的方法，输入是图像像素，输出是6自由度的变换矩阵和每个像素的深度估计。用视频连续帧的不同视角的几何信息作为监督信号，训练一种端到端的单目图像深度估计和和相机运动估计的框架。如图1所示，该框架包括一个用于单一视角深度估计的Depth CNN，以及用于连续帧间运动估计的Pose CNN。
    在先前的相关研究工作中，文献【44】建议用视图合成(view synthesis)作为metric，文献【10】在端到端框架中处理了标定的、多视图3D的情况。上述内容使我们受到了很大的启发。我们的方法是无监督的，并可简单地用未经人工标定（甚至无相机运动信息）的视频序列训练。
    这种用multi-view observations来学习single-view depth及帧间转移的方法是基于预测结果和多视角观察之间的几何一致性(geometrically consistent)。



[2 相关研究工作]
    如何让计算机从单张图片推断3D结构一直是计算机视觉领域的难点和热点。现有的一些CNN+Depth或CNN+SLAM的工作大概可以分为：1)直接利用深度图进行监督学习；2)利用帧间转移的ground-truth pose进行监督学习。然而，这类监督学习的方法需要的数据成本较高，难以获取大规模训练数据。在小数据集上训练，往往导致这些方法在没有见过的场景下工作效果并不好，尤其是自动驾驶场景下面临着复杂多变的道路场景，这些监督学习的方法都不太适用。而本方法采用了无监督的方法针对视频数据进行训练，从而对单张图片的深度以及连续帧之间的车辆运动进行估计，可以对大量已知相机内参的视频数据进行训练，为CNN在自动驾驶领域的应用带来新的启发。

Structure from motion

Warping-based view synthesis

Learning single-view 3D from registered 2D views

Unsupervised/Self-supervised learning from video

https://www.zhihu.com/people/guohengkai/activities



[3 方法]
    我们在此提出一个框架，用未标注的视频序列对单视图的depth-CNN和相机位姿估计的pose-CNN联合训练。尽管两个网络是联合训练的，但深度模型和位姿估计模型可在测试时独立使用。训练的样本是由运动的相机拍摄的视频序列。虽然我们的训练流程对场景一定程度的移动是鲁棒的，但我们仍假设，我们感兴趣的场景大多为静态的。

[3.1 视图合成作为监督 View synthesis as supervision]
    用CNN进行深度和位姿估计的监督信号的关键，来源于novel view synthesis任务：输入一个场景，输出一个由不同相机位姿合成的新图片。我们可以

































