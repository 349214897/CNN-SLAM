Unsupervised learning of depth and ego-motion from video

[摘要]
    我们提出了一个无监督学习框架，它可从非结构化（即：未进行数据标注）的单目视频序列中估计出单目场景的深度和相机位姿。和最近[10,14,16]的工作相同，我们使用了一个端到端的学习方法，用视场合成(view synthesis)作为信号监督。和之前的工作相比，我们的方法是完全无监督的，只需要用单目视频序列训练即可。我们的方法运用了单视图深度(single-view depth)和多视图位姿网络(multi-view pose networks)，并用计算出的深度和位姿将临近的视图扭曲到目标上，作为loss(with a loss based on warping nearby views to the target using the computed depth and pose)。网络在训练的时候通过loss耦合，但在测试时可以独立应用。在KITTI数据组以实验为依据的评估也证明了我们方法的有效性：1）单目深度估计效果很好，其效果与使用ground-truth位姿/深度的监督学习方法的效果差不多。2）该方法的位姿估计效果和基于SLAM的位姿估计效果差不多。


[1 引言]
    人类非常擅长判断自己的运动状态和周围三维场景结构，科学上的一种解释是我们在日常生活中经历了大量的视觉场景，并通过该过程在场景理解方面积累了丰富的经验。基于已有的经验，人们在新的未知场景下（甚至对单张单目图片）也能对场景有正确的结构化理解。而基于纯几何的SLAM等三维重建方法由于没有这种机器学习能力，因此在深度判断等场景理解任务上就必然会有无法避免的缺陷。
    在本工作中，如图1所示，我们模仿(mimic)上述过程，通过用图像序列训练网络，使其拥有估计相机运动和场景结构的能力。我们使用了一个端到端(end-to-end)的方法，输入是图像像素，输出是6自由度的变换矩阵和每个像素的深度估计。在先前的相关研究工作中，文献【44】建议用视图合成(view synthesis)作为metric，文献【10】在端到端框架中处理了标定的、多视图3D的情况。上述内容使我们受到了很大的启发。我们的方法是无监督的，并可简单地用未经人工标定（甚至无相机运动信息）的视频序列训练。

[2 相关研究工作]
给定一张图像，人类可以根据以往的视觉经验推断出 3D 景深，而如何让计算机从单张图片推断 3D 结构一直是计算机视觉领域的难点和热点。现有的一些 CNN+Depth 或者 CNN+SLAM 的工作大概可以分为：直接利用深度图进行监督学习，以及利用帧间转移的 ground-truth pose 进行监督学习。然而，这类监督学习的方法需要的数据成本较高，难以获取大规模训练数据。在小数据集上训练，往往导致这些方法在没有见过的场景下并不 work，给人的感觉是 CNN 与深度估计以及 SLAM 的结合都停留在实验室和 paper 上，尤其是自动驾驶场景下面临着复杂多变的道路场景，这些监督学习的方法都不太适用。而今天分享的这篇论文，采用了无监督的方法针对视频数据进行训练，从而对单张图片的深度以及连续帧之间的车辆运动进行估计，可以对大量已知相机内参的视频数据进行训练，为 CNN 在自动驾驶领域的应用带来的新的启发。
















