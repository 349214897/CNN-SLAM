# CNN-SLAM
The combination of CNN and SLAM, aiming to achieve a better result of traditional geometric-based SLAM architecture.

             CNN-SLAM:实时稠密单目SLAM及深度估计
【摘要】
	鉴于最近深度卷积神经网络(CNN)在深度估计方面的研究有不错的进展，这篇论文研究如何用CNN和单目相机重建出准确且稠密的三维场景。我们提出了一个方法，它可让CNN预测的稠密深度地图与单目SLAM估计的深度图自然融合。该方法在单目SLAM失效的时候（如低纹理区域）仍然能够很好地工作。我们展示了用深度预测来估计重建三维场景的绝对尺度，以解决单目SLAM无法得到绝对尺度这一主要缺陷。最后，我们提出了一个框架，它可将单幅图像中提取的语义标签和稠密SLAM高效地融合在一起，达到从单视角语义一致性地重建场景的效果。在两个数据集上的测试表明我们提出的算法有很高的准确性和很强的鲁棒性。


Migration:
Switching code to OpenCV3 can be done following the official guide at http://docs.opencv.org/master/db/dfa/tutorial_transition_guide.html. 

【1 引言-部分内容】
	传统SLAM有几点缺陷：
		1.工作范围小。
		2.基于主动感知的SLAM方法无法在日光场景下表现很差。
		3.单目SLAM无法获得绝对尺度，会存在尺度漂移现象。若无法获得绝对尺度，则在VR/AR和机器人应用中都会遇到很大问题。
		4.单目SLAM无法在纯旋转情况下工作，因为此时的stereo-baseline丢失，无法进行立体估计。
	近来，通过机器学习方法基于单幅图像估计场景深度的研究取得了新进展。其中，深度卷积神经网络(CNN)表现出色。其优点是可以得到高分辨的、绝对尺度的深度估计图，甚至在特征点稀疏或repetitive patterns的情况下也表现出色。由于场景的绝对深度可以通过样本训练学习得到，因此不需要基于场景的模型假设，也不需要几何上的约束。然而，此方法的缺陷是虽然深度预测在全局上是准确的，但在深度图中物体的边界处是局部模糊的，这样会丧失物体深度、形状的细节信息。
	我们提出该方法的主要思想是：将深度估计和单目SLAM结合。为了解决深度图中的模糊边缘问题(blurred depth borders)，我们将CNN预测的深度图作为稠密重建的初始猜测(initial guess)，并连续地通过直接法SLAM来优化（依靠small-baseline stereo matching，该方法和参考文献【4】的LSD-SLAM相似，LSD-SLAM是典型的直接法)。非常重要的一点是，small-baseline stereo matching拥有可以改善边缘区域深度估计的潜力。与此同时，CNN预测的深度图有绝对尺度，可为位姿估计提供更多约束条件，显著提高了位姿估计、路径和重建场景的精度。而且由于本方法可克服纯旋转等恶劣情况，因此tracking的稳定性大大增强。另外，该框架可在PC上实时运行，CPU和GPU同时运算，用GPU来执行CNN的深度估计，用CPU来执行深度优化。
	除可用于深度估计之外，CNN还可成功地用于其他高维回归(high-dimension regression)的任务，其中一个典型的例子就是语义分割(semantic segmentation)。我们基于这种方法提出了CNN-SLAM的扩展版本，即用pixel-wise labels将语义labels和稠密SLAM准确一致且高效率地融合在一起。

【2 相关研究工作】
	SLAM按传感器分可分为基于深度相机的和基于单目相机的，按方法分可分为直接法和特征点法。
	对于单目SLAM，ORB可以说是state-of-the-art的。
	由于深度学习的发展，从单视角进行深度估计的研究得到了越来越多的重视。

【3 单目语义SLAM】
	我们在该部分介绍所提出的方法，图2显示了该算法的框架和流程。我们使用基于关键帧的SLAM范例，特别的是，我们用参考文献【4 LSD-SLAM】的方法（直接法、半稠密）作为baseline。在该方法中，关键帧作为其子集，从视觉上显著的（visually distinct)帧集合中选出。这些关键帧的位姿将参与全局优化，优化各自的位姿。与此同时，每输入一帧图像，程序就通过该帧与其最近关键帧的transformation来估计该帧的相机位姿。
	为保持算法能以高帧率运行，我们只预测关键帧的深度图（用CNN生成）。特别地，如果当前的位姿估计和现有关键帧相差很多，则基于当前帧生成一个新的关键帧，并用CNN生成其深度图。另外，我们还通过对每个深度预测图进行像素级(pixel-wise)的置信(confidence)衡量，生成一个uncertainty map。由于一般情况下，SLAM用的相机和为产生CNN训练图像而用的相机并不相同，我们提出了将深度图规范化（normalized）的方法，这样对不同内参的相机拍摄的视频流都有较好的鲁棒性。当进行语义label fusion时，我们用另一个卷积网络对输入帧进行语义分割的预测。最后，生成关键帧的位姿图，来全局优化它们的相对位姿。
	该框架很重要的一个环节（同时也是我们的主要贡献），就是通过small-baseline stereo matching来改善CNN预测的关键帧深度图。我们通过在关键帧和对应输入帧之间最小化color consistency来实现。值得一提的是，深度图中的边缘区域将主要通过该环节得到优化，epipolar matching可提供更高的准确性。这部分内容将在3.3和3.4中介绍。优化后的深度的传播方式受各深度值的不确定性的影响，这种不确定性由我们特地提出的置信衡量方法而得出（在3.3中定义）。该框架的每一个步骤都将在以下小节中详细阐述。

【3.1 相机位姿估计】
	相机位姿估计的方法受LSD-SLAM（参考文献4）的关键帧方法的启发而得。系统保存了一系列关键帧k1, k2, ... kn 并将它们作为结构元素，参与SLAM重建。每一个关键帧ki都对应着相应的关键帧位姿T_ki，深度图D_ki和深度不确定性地图U_ki。与【4】不同的是，我们得到的深度图是稠密的，它由基于CNN的深度预测产生（详见3.2节）。不确定性地图估计了每个深度值的confidence。【4】将不确定性地图初始化为一个大的、恒定值。与它不同的是，我们根据深度预测的measured confidence初始化不确定性地图。我们将深度图的元素标记为u=(x,y)。
	对于每一帧t，我们的目标是估计当前的相机位姿T^ki_t=[R_t,t_t]矩阵，即离t最近的关键帧ki到t的变换。它由3*3的旋转矩阵R_t和3D平移向量t_t组成。该变换由最小化当前帧图片L_t和最近关键帧L_ki的光度残差(photometric residual)，并通过高斯-牛顿优化目标函数得到（详见公式【1】）。ρ是Huber范数(Huber norm)，σ是衡量残差不确定性的函数（详见【4】）。r是光度残差，它由公式【2】定义。
	由于深度图是稠密的，因此从效率上考虑，我们仅针对颜色梯度高的区域中像素的一个子集来计算光度残差，由u^~表示。在公式【2】中，pi表示将3D点投影到2D图像上的投影函数，见公式【3】。V_ki表示从关键帧深度图计算生成vertex map中的3D元素，见公式【4】。K是相机的内参矩阵。
	一旦得到了T^ki_t，当前帧在世界坐标系下的的相机位姿即可用T_t = T^ki_t * T_ki 计算。

【3.2 基于CNN的深度估计和语义分割】
	每当生成一个新的关键帧时，CNN便随之产生一幅对应的深度预测图。我们使用的深度预测方法属于目前最高水平（文献【16】中提出的方法），它基于Residual Network (ResNet)延伸出一个全卷积网络。该架构的第一部分基于ResNet-50（文献【9】），由ImageNet训练生成的权重初始化该部分。框架的第二部分将由ResNet-50提出的last pooling和fully connected layers替换为一系列上采样的blocks，这些blocks由unpooling和卷积层组成。在上采样之后，drop-out is applied before a final convolutional layer which outputs a 1-channel output map representing the predicted depth map. The loss function is based on the reverse Huber function（文献【16】）.
	我们借鉴其他用相同的架构来进行深度预测、语义分割的成功范例（参考文献【3，29】），也训练了该网络，用于从RGB图像中预测像素级的语义label。我们修改了网络，使它的输出通道数和类别数相同，并用了soft-max层和cross-entropy loss function，通过反向传播和Stochastic Gradient Descent(SGD)最小化此loss function。需要指明的是，虽然原则上可以使用任何语义分割算法，但我们此工作的目的是展现帧级别的分割图是如何成功地和单目SLAM框架融合在一起的（详见3.5节）。

【3.3 生成关键帧和位姿图优化】
	使用预先训练的CNN来进行深度预测的局限是，如果SLAM所用传感器的内参和拍摄训练集图像的相机的内参不同，那么3D重建时的绝对尺度估计必然不准确。为解决该问题，我们用当前相机的焦距f_cur和训练集所用相机的焦距f_tr的比值，通过CNN调整深度（公式【5】）。D^~_ki是通过当前关键帧L_i由CNN直接退化得到的深度图。
	图3显示了用公式【5】校正的有效性。如图所示，经过校正后深度图和轨迹的准确性都明显提升。
	另外，我们将每个深度图D_ki和不确定性地图U_ki联系在一起。在参考文献【4】中，将该不确定地图的每个元素初始化为一个大的常数。由于CNN可在不依赖于任何temporal regulation的情况下，产生每帧对应的深度图，因此我们通过计算当前深度地图和距它最近的关键帧上的对应点的差别，计算出置信值，从而将不确定地图初始化。这样以来，置信值衡量了在不同的每个预测的深度值


following【4】之后的内容：
sigma^2_p是白噪声方差，用于增加传播不确定度(propagated uncertainty)。之后，根据权重规则(weighted scheme)，将两个深度图和不确定图融合（公式【8】【9】）。
    最终，在图(graph)中找到与新加入关键帧视场相似（即位姿相近）的一系列关键帧，根据这些关键帧创建新的边缘，并通过这些新创建的边缘更新位姿图(Finally, the pose graph is also updated at each new key-frame, by creating new edges with the key-frames already present in the graph that share a similar field of view(i.e., having a small relative pose) with the newly added key-frame)。每次关键帧的位姿都通过位姿图优化（文献【14】）来得到全局的优化。

【3.4 帧级深度优化】
    该步骤的目标是基于每个new-frame的深度估计图不断优化当前活跃的关键帧的深度图。我们用small baseline stereo matching的方法（文献【5】中的半稠密框架），基于沿极线的5-pixel匹配，遍历当前帧t的每个像素生成D_t和U_t。之后，基于相机位姿估计T^ki_t，通过关键帧k_i对D_t和U_t进行校准。
    估计生成的D_t和U_t将根据下式直接与最近关键帧k_i的D_ki和U_ki融合（公式【10】【11】）。













