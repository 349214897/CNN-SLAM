# CNN-SLAM
The combination of CNN and SLAM, aiming to achieve a better result of traditional geometric-based SLAM architecture.

             CNN-SLAM:实时稠密单目SLAM及深度估计
【摘要】
	鉴于最近深度卷积神经网络(CNN)在深度估计方面的研究有不错的进展，这篇论文研究如何用CNN和单目相机重建出准确且稠密的三维场景。我们提出了一个方法，它可让CNN预测的稠密深度地图与单目SLAM估计的深度图自然融合。该方法在单目SLAM失效的时候（如低纹理区域）仍然能够很好地工作。我们展示了用深度预测来估计重建三维场景的绝对尺度，以解决单目SLAM无法得到绝对尺度这一主要缺陷。最后，我们提出了一个框架，它可将单幅图像中提取的语义标签和稠密SLAM高效地融合在一起，达到从单视角语义一致性地重建场景的效果。在两个数据集上的测试表明我们提出的算法有很高的准确性和很强的鲁棒性。


Migration:
Switching code to OpenCV3 can be done following the official guide at http://docs.opencv.org/master/db/dfa/tutorial_transition_guide.html. 

【1 引言-部分内容】
	传统SLAM有几点缺陷：
		1.工作范围小。
		2.基于主动感知的SLAM方法无法在日光场景下表现很差。
		3.单目SLAM无法获得绝对尺度，会存在尺度漂移现象。若无法获得绝对尺度，则在VR/AR和机器人应用中都会遇到很大问题。
		4.单目SLAM无法在纯旋转情况下工作，因为此时的stereo-baseline丢失，无法进行立体估计。
	近来，通过机器学习方法基于单幅图像估计场景深度的研究取得了新进展。其中，深度卷积神经网络(CNN)表现出色。其优点是可以得到高分辨的、绝对尺度的深度估计图，甚至在特征点稀疏或repetitive patterns的情况下也表现出色。由于场景的绝对深度可以通过样本训练学习得到，因此不需要基于场景的模型假设，也不需要几何上的约束。然而，此方法的缺陷是虽然深度预测在全局上是准确的，但在深度图中物体的边界处是局部模糊的，这样会丧失物体深度、形状的细节信息。
	我们提出该方法的主要思想是：将深度估计和单目SLAM结合。为了解决深度图中的模糊边缘问题(blurred depth borders)，我们将CNN预测的深度图作为稠密重建的初始猜测(initial guess)，并连续地通过直接法SLAM来优化（依靠small-baseline stereo matching，该方法和参考文献【4】的LSD-SLAM相似，LSD-SLAM是典型的直接法)。非常重要的一点是，small-baseline stereo matching拥有可以改善边缘区域深度估计的潜力。与此同时，CNN预测的深度图有绝对尺度，可为位姿估计提供更多约束条件，显著提高了位姿估计、路径和重建场景的精度。而且由于本方法可克服纯旋转等恶劣情况，因此tracking的稳定性大大增强。另外，该框架可在PC上实时运行，CPU和GPU同时运算，用GPU来执行CNN的深度估计，用CPU来执行深度优化。
	除可用于深度估计之外，CNN还可成功地用于其他高维回归(high-dimension regression)的任务，其中一个典型的例子就是语义分割(semantic segmentation)。我们基于这种方法提出了CNN-SLAM的扩展版本，即用pixel-wise labels将语义labels和稠密SLAM准确一致且高效率地融合在一起。

【2 相关研究工作】
	SLAM按传感器分可分为基于深度相机的和基于单目相机的，按方法分可分为直接法和特征点法。
	对于单目SLAM，ORB可以说是state-of-the-art的。
	由于深度学习的发展，从单视角进行深度估计的研究得到了越来越多的重视。

【3 单目语义SLAM】
	我们在该部分介绍所提出的方法，图2显示了该算法的框架和流程。我们使用基于关键帧的SLAM范例，特别的是，我们用参考文献【4 LSD-SLAM】的方法（直接法、半稠密）作为baseline。在该方法中，关键帧作为其子集，从视觉上显著的（visually distinct)帧集合中选出。这些关键帧的位姿将参与全局优化，优化各自的位姿。与此同时，每输入一帧图像，程序就通过该帧与其最近关键帧的transformation来估计该帧的相机位姿。
	为保持算法能以高帧率运行，我们只预测关键帧的深度图（用CNN生成）。特别地，如果当前的位姿估计和现有关键帧相差很多，则基于当前帧生成一个新的关键帧，并用CNN生成其深度图。另外，我们还通过对每个深度预测图进行像素级(pixel-wise)的置信(confidence)衡量，生成一个uncertainty map。由于一般情况下，SLAM用的相机和为产生CNN训练图像而用的相机并不相同，我们提出了将深度图规范化（normalized）的方法，这样对不同内参的相机拍摄的视频流都有较好的鲁棒性。当进行语义label fusion时，我们用另一个卷积网络对输入帧进行语义分割的预测。最后，生成关键帧的位姿图，来全局优化它们的相对位姿。
	该框架很重要的一个环节（同时也是我们的主要贡献），就是通过small-baseline stereo matching来改善CNN预测的关键帧深度图。我们通过在关键帧和对应输入帧之间最小化color consistency来实现。值得一提的是，深度图中的边缘区域将主要通过该环节得到优化，epipolar matching可提供更高的准确性。这部分内容将在3.3和3.4中介绍。优化后的深度的传播方式受各深度值的不确定性的影响，这种不确定性由我们特地提出的置信衡量方法而得出（在3.3中定义）。该框架的每一个步骤都将在以下小节中详细阐述。

【3.1 相机位姿估计】
	相机位姿估计的方法受LSD-SLAM（参考文献4）的关键帧方法的启发而得。系统保存了一系列关键帧k1, k2, ... kn 并将它们作为结构元素，参与SLAM重建。每一个关键帧ki都对应着相应的关键帧位姿T_ki，深度图D_ki和深度不确定性地图U_ki。与【4】不同的是，我们得到的深度图是稠密的，它由基于CNN的深度预测产生（详见3.2节）。不确定性地图估计了每个深度值的confidence。【4】将不确定性地图初始化为一个大的、恒定值。与它不同的是，我们根据深度预测的measured confidence初始化不确定性地图。我们将深度图的元素标记为u=(x,y)。
	对于每一帧t，我们的目标是估计当前的相机位姿T^ki_t=[R_t,t_t]矩阵，即离t最近的关键帧ki到t的变换。它由3*3的旋转矩阵R_t和3D平移向量t_t组成。该变换由最小化当前帧图片L_t和最近关键帧L_ki的光度残差(photometric residual)，并通过高斯-牛顿优化目标函数得到（详见公式【1】）。ρ是Huber范数(Huber norm)，σ是衡量残差不确定性的函数（详见【4】）。r是光度残差，它由公式【2】定义。
	由于深度图是稠密的，因此从效率上考虑，我们仅针对颜色梯度高的区域中像素的一个子集来计算光度残差，由u^~表示。在公式【2】中，pi表示将3D点投影到2D图像上的投影函数，见公式【3】。V_ki表示从关键帧深度图计算生成vertex map中的3D元素，见公式【4】。K是相机的内参矩阵。
	一旦得到了T^ki_t，当前帧在世界坐标系下的的相机位姿即可用T_t = T^ki_t * T_ki 计算。

【3.2 基于CNN的深度估计和语义分割】
	每当生成一个新的关键帧时，CNN便随之产生一幅对应的深度预测图。我们使用的深度预测方法属于目前最高水平（文献【16】中提出的方法），它基于Residual Network (ResNet)延伸出一个全卷积网络。该架构的第一部分基于ResNet-50（文献【9】），由ImageNet训练生成的权重初始化该部分。框架的第二部分将由ResNet-50提出的last pooling和fully connected layers替换为一系列上采样的blocks，这些blocks由unpooling和卷积层组成。在上采样之后，drop-out is applied before a final convolutional layer which outputs a 1-channel output map representing the predicted depth map. The loss function is based on the reverse Huber function（文献【16】）.
	我们借鉴其他用相同的架构来进行深度预测、语义分割的成功范例（参考文献【3，29】），也训练了该网络，用于从RGB图像中预测像素级的语义label。我们修改了网络，使它的输出通道数和类别数相同，并用了soft-max层和cross-entropy loss function，通过反向传播和Stochastic Gradient Descent(SGD)最小化此loss function。需要指明的是，虽然原则上可以使用任何语义分割算法，但我们此工作的目的是展现帧级别的分割图是如何成功地和单目SLAM框架融合在一起的（详见3.5节）。

【3.3 生成关键帧和位姿图优化】
	使用预先训练的CNN来进行深度预测的局限是，如果SLAM所用传感器的内参和拍摄训练集图像的相机的内参不同，那么3D重建时的绝对尺度估计必然不准确。为解决该问题，我们用当前相机的焦距f_cur和训练集所用相机的焦距f_tr的比值，通过CNN调整深度（公式【5】）。D^~_ki是通过当前关键帧L_i由CNN直接退化得到的深度图。
	图3显示了用公式【5】校正的有效性。如图所示，经过校正后深度图和轨迹的准确性都明显提升。
	另外，我们将每个深度图D_ki和不确定性地图U_ki联系在一起。在参考文献【4】中，将该不确定地图的每个元素初始化为一个大的常数。由于CNN可在不依赖于任何temporal regulation的情况下，产生每帧对应的深度图，因此我们通过计算当前深度地图和距它最近的关键帧上的对应点的差别，计算出置信值，从而将不确定地图初始化。这样以来，置信值衡量了在不同的每个预测的深度值


following【4】之后的内容：
sigma^2_p是白噪声方差，用于增加传播不确定度(propagated uncertainty)。之后，根据权重规则(weighted scheme)，将两个深度图和不确定图融合（公式【8】【9】）。
    最终，在图(graph)中找到与新加入关键帧视场相似（即位姿相近）的一系列关键帧，根据这些关键帧创建新的边缘，并通过这些新创建的边缘更新位姿图(Finally, the pose graph is also updated at each new key-frame, by creating new edges with the key-frames already present in the graph that share a similar field of view(i.e., having a small relative pose) with the newly added key-frame)。每次关键帧的位姿都通过位姿图优化（文献【14】）来得到全局的优化。

【3.4 帧级深度优化】
    该步骤的目标是基于每个new-frame的深度估计图不断优化当前活跃的关键帧的深度图。我们用small baseline stereo matching的方法（文献【5】中的半稠密框架），基于沿极线的5-pixel匹配，遍历当前帧t的每个像素生成D_t和U_t。之后，基于相机位姿估计T^ki_t，通过关键帧k_i对D_t和U_t进行校准。
    估计生成的D_t和U_t将根据下式直接与最近关键帧k_i的D_ki和U_ki融合（公式【10】【11】）。
    重要的是，由于关键帧和稠密深度图有关，因此该过程可稠密地执行，即关键帧的每个元素都可得到优化，而不是像文献【5】那样只优化梯度高的区域的深度值。由于低纹理区域的深度值往往有较高的不确定度（即U_t中对应位置的值较高），因此本方法可自动地、有选择性地优化：对于高梯度区域，每一帧都参与优化该区域的深度；对于低纹理区域，其深度值将逐渐稳定在CNN估计的深度值附近，而不会受深度值不确定性的影响。图3-(c)展示了该深度图优化方法的有效性。

【3.5 三维模型和语义标签的全局融合】
    相机获取的一系列关键帧可融合在一起，实现三维场景重建。由于CNN经过训练后可提供语义标签和深度图，因此语义信息也可以与全局三维场景的每个元素联系起来，我们将语义信息和三维模型的融合过程称为语义标签融合(semantic label fusion)。
    在我们提出的框架中，我们用文献【27】提出的实时算法，该算法旨在渐进地将深度图和RGB-D序列中每帧生成的connected component map融合在一起。该方法利用Global Segmentation Model (GSM)，随着时间的推移，将标签平均分配到每个3D元素，因此该方法对帧级的分割中的噪声是鲁棒的。在我们的框架中，我们将位姿估计作为算法的输入。这是因为相机位姿是由单目SLAM估计的，而输入的深度图只与一系列捕获的关键帧有关。我们使用语义分割图，而不是文献【27】中的connected component maps。该框架实现的效果是：根据新加入的关键帧渐进地重建三维场景，场景中的每个3D元素都关联了一个语义类别。在此之前，我们在训练CNN时使用了这些语义类别。

【4 评估】
    在这一小节，我们对tracking的精度、三维重建的精度、纯旋转情况下的鲁棒性及语义标签融合进行评估。我们在实现CNN-SLAM时，CNN网络的输入/输出分辨率为304*228，但输入帧和预测的深度图的分辨率都先转换为320*240，并将它们作为其余所有阶段的输入。基于CNN的深度预测和语义分割在GPU上运行，算法中的其余部分都在CPU上以两个线程运行，基于这种架构，CNN-SLAM可实时运行。其中，一个线程用于帧级处理（相机位姿估计和深度优化），另一个线程用于关键帧相关处理（关键帧初始化、位姿图优化和全局语义标签融合）。
    我们使用ICL-NUIM（合成的）和TUM RGB-D SLAM（由Kinect捕获）两个数据集测试。这两个数据集都提供了相机路径和深度图的ground truth。在所有的实验中，我们都用NYU Depth v2数据集的室内场景训练CNN，得到训练好的CNN模型，并用此模型测试该网络在从未见过的环境下的泛化性能(test the generalization capability of the network to unseen environments)。另外，NYU Depth v2数据集包含了深度的ground truth和帧级的语义标签注释，而这恰恰是语义标签融合所必须的。特别需要说明的是，我们在官方的train split of the labeled subset上训练语义分割网络，而用原始的NYU数据集（里面的图片更多）训练深度网络，如文献【16】所述。语义注释由4个超级类(super-class)组成：floor, vertical structure, large structure/furniture, small structure. 值得说明的是，由于相机传感器、视角和场景不同，训练数据集的设置与评价CNN-SLAM的数据集设置有所不同。例如，NYU Depth v2数据集包含了很多起居室、厨房和卧室的图片，而TUM RGB-D SLAM数据集中更多的是办公室场景，例如书桌、物体和人物。因此这些起居室、厨房和卧室等场景是TUM RGB-D SLAM数据集中没有的。

【4.1 CNN-SLAM与当前最高水平SLAM的对比】
    将CNN-SLAM与当前单目SLAM最高水平的两个框架：LSD-SLAM（直接法）和ORB-SLAM（特征点法）作对比。为保持完整性，我们还和REMODE（文献【23】，REMODE是单目稠密深度图估计的当前最高水平）作对比。REMODE可根据作者提供的github代码来实现。最后，我们也将CNN-SLAM和文献【16】的方法作对比。【16】将CNN预测的深度图作为基于深度的SLAM的输入（基于点的融合，文献【11,27】，当前基于深度SLAM的最高水平），其实现基于文献【27】提供的代码。
    Given the ambiguity of monocular SLAM approaches to estimate absolute scale, we also evaluate LSD-SLAM by bootstrapping its initial scale using the ground-truth depth map, as done in the evaluation in [4, 20]. As for REMODE, since it requires as input the camera pose estimation at each frame, we use the trajectory and key-frames estimated by LSD-SLAM with bootstrapping.

LSD-SLAM:	https://www.github.com/tum-vision/lsd_slam
ORB-SLAM2:	https://www.github.com/raulmur/ORB_SLAM2
REMODE:		https://www.github.com/uzh-rpg/rpg_open_remode
文献【27】:	https://campar.in.tum.de/view/Chair/ProjectInSeg

    按照文献【26】中提出的评价方法，表1列出了基于绝对轨迹误差(Absolute Trajectory Error, ATE)的相机位姿准确性评估（计算方均根）。另外，我们通过计算深度误差小于10%的点的比例，评估了重建准确性和重建密度。从表1可看出，CNN-SLAM优于其他方法，而且三维重建的准确性、稠密性也大大提高。
    深度图估计的准确性对比如图4所示。从图中可看出，CNN-SLAM能明显改善CNN生成深度估计图中边缘模糊的情况，且对低纹理场景也能正常工作。

【4.2 纯旋转场景下的准确性评估】
    效果对比如图5所示。我们的方法可在纯旋转场景下正常工作，而LSD-SLAM会有显著的噪声；ORB-SLAM根本无法工作，因为在初始化时若场景是纯旋转的，则无法获得初始化必须的baseline，因此无法完成初始化。

【4.3 三维重建场景和语义标签的融合】
    图6给出了3个融合的例子，绿色的是估计的相机轨迹。据我们所知，这是该领域第一个用单目相机，将三维重建场景和语义信息融合的实验。其他相关的实验结果和分析在补充材料中。

【5 总结】
    CNN-SLAM为单目相机进行场景理解提供了新思路。今后的研究方向可以是通过深度预测进行回环检测，即通过几何方法优化深度图，提高深度估计的准确性。



毕设或读研可做的另一件事：
用已知的速度或尺度标定单目SLAM的scale，和CalmCar的工作内容类似。


---------------------------------------------------------------
Unsupervised Learning of Depth and Ego-Motion from Video

【摘要】
    我们提出了用于“单目相机估计深度和运动位姿”的无监督学习方法，它可从未标记数据的视频训练集进行无监督学习，并有很好的表现。

最近，UCBerkeley 的研究人员撰文介绍了他们在计算机视觉研究中的最新成果：利用单幅图片进行 3D推断的计算模型。这种方法在无人驾驶汽车等领域具有很大潜力，同时，研究人员认为构建新模型的原则也可以应用到机器学习的其他领域中。












