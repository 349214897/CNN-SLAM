Unsupervised learning of depth and ego-motion from video

[摘要]
    我们提出了一个无监督学习框架，它可从非结构化（即：未进行数据标注）的单目视频序列中估计出单目场景的深度和相机位姿。和最近[10,14,16]的工作相同，我们使用了一个端到端的学习方法，用视场合成(view synthesis)作为信号监督。和之前的工作相比，我们的方法是完全无监督的，只需要用单目视频序列训练即可。我们的方法运用了单视图深度(single-view depth)和多视图位姿网络(multi-view pose networks)，并用计算出的深度和位姿将临近的视图扭曲到目标上，作为loss(with a loss based on warping nearby views to the target using the computed depth and pose)。网络在训练的时候通过loss耦合，但在测试时可以独立应用。在KITTI数据组以实验为依据的评估也证明了我们方法的有效性：1）单目深度估计效果很好，其效果与使用ground-truth位姿/深度的监督学习方法的效果差不多。2）该方法的位姿估计效果和基于SLAM的位姿估计效果差不多。


[1 引言]
    人类非常擅长判断自己的运动状态和周围三维场景结构，科学上的一种解释是我们在日常生活中经历了大量的视觉场景，并通过该过程在场景理解方面积累了丰富的经验。基于已有的经验，人们在新的未知场景下（甚至对单张单目图片）也能对场景有正确的结构化理解。而基于纯几何的SLAM等三维重建方法由于没有这种机器学习能力，因此在深度判断等场景理解任务上就必然会有无法避免的缺陷。
    在本工作中，如图1所示，我们模仿(mimic)上述过程，通过用图像序列训练网络，使其拥有估计相机运动和场景结构的能力。我们使用了一个端到端(end-to-end)的方法，输入是图像像素，输出是6自由度的变换矩阵和每个像素的深度估计。在先前的相关研究工作中，文献【44】建议用视图合成(view synthesis)作为metric，文献【10】在端到端框架中处理了标定的、多视图3D的情况。上述内容使我们受到了很大的启发。我们的方法是无监督的，并可简单地用未经人工标定（甚至无相机运动信息）的视频序列训练。



















